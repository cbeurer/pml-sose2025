\documentclass[a4paper]{article}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{blindtext} % Package to generate dummy text
\usepackage{charter} % Use the Charter font
\usepackage[utf8]{inputenc} % Use UTF-8 encoding
\usepackage[T1]{fontenc}
\usepackage{mathpazo}
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage{amsthm, amsmath, amssymb, amsfonts} % Mathematical typesetting
\usepackage{float} % Improved interface for floating objects
\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{graphicx, multicol} % Enhanced support for graphics
\usepackage{xcolor} % Driver-independent color extensions
\usepackage{pseudocode} % Environment for specifying algorithms in a natural way
\usepackage{datetime} % Uses YEAR-MONTH-DAY format for dates

\addtolength{\hoffset}{-2.25cm}
\addtolength{\textwidth}{4.5cm}
\addtolength{\voffset}{-3.25cm}
\addtolength{\textheight}{5cm}

\setlength{\parskip}{1ex}
\setlength{\parindent}{0in}

\DeclareUnicodeCharacter{03B1}{$\alpha$}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{}\renewcommand{\headrulewidth}{0pt} % Blank out the default header
\fancyfoot[L]{} % Custom footer text
\fancyfoot[C]{} % Custom footer text
\fancyfoot[R]{\thepage} % Custom footer text
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}

% define new commands
\newcommand{\Real}{{\mathbb R}}
\newcommand{\Normal}[3]{{\mathcal N} \left({#1};{#2},{#3}\right)}
\newcommand{\Gauss}[3]{{\mathcal G} \left({#1};{#2},{#3}\right)}
\newcommand{\NormalStandard}[1]{{\mathcal N} \left({#1}\right)}
\newcommand{\GaussStandard}[1]{{\mathcal G} \left({#1}\right)}
\newcommand{\NormalCDF}[3]{\Phi \left({#1};{#2},{#3}\right)}
\newcommand{\NormalStandardCDF}[1]{\Phi \left({#1}\right)}
\newcommand{\Bernoulli}[2]{{\mathrm{Ber}} \left({#1};{#2}\right)}
\newcommand{\Ber}[2]{{\mathcal B} \left({#1};{#2}\right)}
\newcommand{\expect}[1]{{\mathbb E \left[ {#1} \right]}}
\newcommand{\bs}[1]{{\boldsymbol{#1}}}
\newcommand{\var}[1]{{\mathbb V \left[ {#1} \right]}}
\newcommand{\dirac}[1]{{\delta \left( {#1} \right)}}
\newcommand{\uniform}[1]{{\mathcal U} \left( {#1} \right)}
\newcommand{\identity}[1]{{{\mathbb{I}} \left( {#1} \right)}}
\newcommand{\incoming}[1]{\widecheck{#1}}
\newcommand{\outgoing}[1]{\widehat{#1}}
\newcommand{\intd}[1]{\ \mathrm{d}{#1}}
\newcommand{\transpose}[1]{{#1}^\top}
% \newcommand{\incoming}[1]{\stackrel{\rightarrow}{#1}}
% \newcommand{\outgoing}[1]{\stackrel{\leftarrow}{#1}}
\newcommand{\neighbours}[1]{{\mathrm{ne} \left( {#1} \right)}}


% define new environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

%----------------------------------------------------------------------------------------

\begin{document}

%-------------------------------
%	TITLE SECTION (do not modify unless you really need to)
%-------------------------------
\fancyhead[C]{}
\hrule \medskip
\begin{minipage}{0.295\textwidth}
    \raggedright
    \hfill\\
    % \footnotesize
    % Start: 24.4.2023\\
    % Return: 19.5.2023 \hfill\\
    % \href{https://classroom.github.com/classrooms/124387260-hpi-artificial-intelligence-teaching-pml-classroom}{https://classroom.github.com}
\end{minipage}
\begin{minipage}{0.4\textwidth}
    \centering
    \large
    Properties of Multidimensional Gaussian Distributions\\
\end{minipage}
\begin{minipage}{0.295\textwidth}
    \raggedleft
    \hfill\\
\end{minipage}
\medskip\hrule
\bigskip
 
\section*{Representations}
In the following, we will derive results for the multi-dimensional Gaussian distribution.
\begin{definition}[Multi-Dimensional Gaussian Distribution]
    Given parameters $\bs{\mu} \in \Real^n$, $\bs{\Sigma} \in \Real^{n \times n}$, $\bs{\tau} \in \Real^n$, and $\bs{P} \in \Real^{n \times n}$, where $\bs{P}$ and $\bs{\Sigma}$ are positive-definite matrices, an $n$-dimensional random variable $\bs{X} \in \Real^n$ is distributed according to a multi-dimensional Gaussian distribution if the density has the form $\Normal{\cdot}{\bs{\mu}}{\boldsymbol{\Sigma}}$ or $\Gauss{\cdot}{\bs{\tau}}{\bs{P}}$ with
    \begin{align}
        \Normal{\bs{x}}{\bs{\mu}}{\bs{\Sigma}} & := \left(2\pi\right)^{-\frac{n}{2}} \left|\bs{\Sigma}\right|^{-\frac{1}{2}} \cdot \exp \left( -\frac{1}{2} \transpose{(\bs{x}-\bs{\mu})} \bs{\Sigma}^{-1} (\bs{x}-\bs{\mu}) \right) \,, \label{eq:Normal_definition} \\
        \Gauss{\bs{x}}{\bs{\tau}}{\bs{P}}      & := \left(2\pi\right)^{-\frac{n}{2}} \left|\bs{P}\right|^{\frac{1}{2}} \cdot \exp \left( -\frac{1}{2} \transpose{\bs{\tau}} \bs{P}^{-1} \bs{\tau} \right) \cdot \exp \left( \transpose{\bs{\tau}} \bs{x} -\frac{1}{2} \transpose{\bs{x}} \bs{P} \bs{x} \right)\,. \label{eq:Gauss_definition}
    \end{align}
    If $\bs{P}$ or $\bs{\Sigma}$ are not of full rank, the inverse is replaced by the pseudo-inverse, the exponent on $2\pi$ changes from $-\frac{n}{2}$ to $-\frac{d}{2}$ where $d$ is the number of non-zero eigenvalues of $\bs{P}$ or $\bs{\Sigma}$, respectively, and the determinant is replaced by the pseudo-determinant defined as the product of all $d$ non-zero eigenvalues.
\end{definition}
Note that both definitions are identical when using the identities
\begin{align}
    \bs{\tau} & = \bs{\Sigma}^{-1}\bs{\mu} & \mbox{and} &  & \bs{P}      & = \bs{\Sigma}^{-1} \,,                                       &  &  & \mbox{or} \label{eq:Normal_Gauss_transformation} \\
    \bs{\mu}  & = \bs{P}^{-1}\bs{\tau}     & \mbox{and} &  & \bs{\Sigma} & = \bs{P}^{-1} \,. \label{eq:Gauss_Normal_transformation}
\end{align}

\section*{Multiplication}
A central theorem that allows us to derive many of our efficient factor update equations is the multi-dimensional Gaussian multiplication theorem.
\begin{theorem}[Gaussian Multiplication Theorem] \label{thm:gaussian_multiplication}
    Given two Gaussian densities $\Gauss{\cdot}{\bs{\tau}_1}{\bs{P}_1}$ and $\Gauss{\cdot}{\bs{\tau}_2}{\bs{P}_2}$ we have
    \begin{align*}
        \Gauss{\bs{x}}{\bs{\tau}_1}{\bs{P}_1} \cdot \Gauss{\bs{x}}{\bs{\tau}_2}{\bs{P}_2} & = \Gauss{\bs{x}}{\bs{\tau}_1 + \bs{\tau}_2}{\bs{P}_1 + \bs{P}_2} \cdot \Normal{\bs{\mu}_1}{\bs{\mu}_2}{\bs{\Sigma}_1 + \bs{\Sigma}_2}\,.
    \end{align*}
\end{theorem}
Thus, when multiplying two Gaussian densities, we obtain a non-normalized Gaussian with the precision-mean $\bs{\tau}$ as the sum of the precision-means of the two factors and the precision matrix $\bs{P}$ as the sum of the precision matrices of the two factors. The normalization constant is the density value at $\bs{0}$ when subtracting both means $\bs{\mu}_1$ and $\bs{\mu}_2$ and summing both covariance matrices $\bs{\Sigma}_1$ and $\bs{\Sigma}_2$ of the factors, respectively. 

\begin{proof}\label{prf:gaussian_multiplication_theorem}
    In order to prove this theorem, we use \eqref{eq:Gauss_definition} to see that 
    \begin{align*}
        \frac{\Gauss{\bs{x}}{\bs{\tau}_1}{\bs{P}_1} \cdot \Gauss{\bs{x}}{\bs{\tau}_2}{\bs{P}_2}}{\Gauss{\bs{x}}{\bs{\tau}_1 + \bs{\tau}_2}{\bs{P}_1 + \bs{P}_2}} & = \left(2\pi\right)^{-\frac{n}{2}} \underbrace{\frac{\left|\bs{P}_1\right|^{\frac{1}{2}} \cdot \left|\bs{P}_2\right|^{\frac{1}{2}}}{\left|\bs{P}_1 + \bs{P}_2\right|^{\frac{1}{2}}}}_S \cdot \underbrace{\frac{\exp \left( -\frac{1}{2} \left( \transpose{\bs{\tau}_1} \bs{P}^{-1}_1 \bs{\tau}_1 + \transpose{\bs{\tau}_2} \bs{P}^{-1}_2 \bs{\tau}_2 \right) \right)}{\exp \left( -\frac{1}{2} \left( \transpose{\left( \bs{\tau}_1 + \bs{\tau}_2 \right)} \left( \bs{P}_1 + \bs{P}_2 \right)^{-1} \left( \bs{\tau}_1 + \bs{\tau}_2 \right) \right) \right)}}_T
    \end{align*}
    Using \eqref{eq:Normal_definition}, we only need to show that $S =  \left|\bs{\Sigma}_1 + \bs{\Sigma}_2\right|^{-\frac{1}{2}}$ and $T = \exp \left( -\frac{1}{2} \transpose{(\bs{\mu}_1-\bs{\mu}_2)} \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} (\bs{\mu}_1-\bs{\mu}_2) \right)$ to prove Theorem \ref{thm:gaussian_multiplication}. First, we see that
    \begin{align*}
        S & = \left| \bs{P}_1 \cdot \left( \bs{P}_1 + \bs{P}_2 \right)^{-1} \cdot \bs{P}_2 \right|^{\frac{1}{2}} = \left| \bs{P}_1 \cdot \left(\bs{P}_2 \cdot \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right) \cdot \bs{P}_1 \right)^{-1} \cdot \bs{P}_2 \right|^{\frac{1}{2}} = \left|\left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1}\right|^{\frac{1}{2}} = \left|\bs{\Sigma}_1 + \bs{\Sigma}_2\right|^{-\frac{1}{2}} \,,
    \end{align*}
    where the second step follows from $\bs{P}^{-1} = \bs{\Sigma}$ (see \eqref{eq:Gauss_Normal_transformation}). Using \eqref{eq:Normal_Gauss_transformation} and \eqref{eq:Gauss_Normal_transformation} we see that $\transpose{\bs{\tau}} \bs{P}^{-1} \bs{\tau} = \transpose{\bs{\mu}} \bs{\Sigma}^{-1} \bs{\mu}$ and $\bs{P}_1 + \bs{P}_2 = \bs{\Sigma}^{-1}_2 \cdot \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right) \cdot \bs{\Sigma}^{-1}_1$ which allows us to rewrite $T$ as 
    \begin{align*}
        T & = \exp \left( -\frac{1}{2} \left( \transpose{\bs{\mu}_1} \bs{\Sigma}^{-1}_1 \bs{\mu}_1 + \transpose{\bs{\mu}_2} \bs{\Sigma}^{-1}_2 \bs{\mu}_2 - \transpose{\left( \bs{\Sigma}^{-1}_1 \bs{\mu}_1 + \bs{\Sigma}^{-1}_2 \bs{\mu}_2 \right)} \bs{\Sigma}_1 \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \bs{\Sigma}_2 \left( \bs{\Sigma}^{-1}_1 \bs{\mu}_1 + \bs{\Sigma}^{-1}_2 \bs{\mu}_2 \right) \right) \right) \\
        & = \exp \left( -\frac{1}{2} \left( \transpose{\bs{\mu}_1} \bs{A} \bs{\mu}_1 - \transpose{\bs{\mu}_1} \bs{B} \bs{\mu}_2 - \transpose{\bs{\mu}_2} \bs{C} \bs{\mu}_1 + \transpose{\bs{\mu}_2} \bs{D} \bs{\mu}_2 \right) \right) \,,
    \end{align*}
    where the matrices $\bs{A}$, $\bs{B}$, $\bs{C}$, and $\bs{D}$ are given by
    \begin{align*}
        \bs{A} & = \bs{\Sigma}^{-1}_1 - \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 \,, \\
        \bs{B} & = \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \,, \\
        \bs{C} & = \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 \,, \\
        \bs{D} & = \bs{\Sigma}^{-1}_2 - \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \,. 
    \end{align*}
    All we need to show is $\bs{A} = \bs{B} = \bs{C} = \bs{D} = \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1}$ to prove the theorem. To show this, we note that 
    \begin{align}
        \bs{\Sigma}_1 + \bs{\Sigma}_2 = \bs{\Sigma}_1 \left( \bs{\Sigma}_1^{-1} + \bs{\Sigma}_2^{-1} \right) \bs{\Sigma}_2 = \bs{\Sigma}_2 \left( \bs{\Sigma}_1^{-1} + \bs{\Sigma}_2^{-1} \right) \bs{\Sigma}_1 \,. \label{eq:Sigma_sum}
    \end{align}
    Thus, we finally have
    \begin{align*}
        \bs{A} & = \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \cdot \left( \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right) \bs{\Sigma}^{-1}_1 - \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 \right) = \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \cdot \left( \mathbf{I} + \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 - \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 \right) = \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \,, \\
        \bs{C} & = \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \bs{\Sigma}_1^{-1} \left( \bs{\Sigma}_1^{-1} + \bs{\Sigma}_2^{-1} \right)^{-1} \bs{\Sigma}_2^{-1} \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 = \bs{\Sigma}^{-1}_2 \left( \bs{\Sigma}_1^{-1} + \bs{\Sigma}_2^{-1} \right)^{-1} \bs{\Sigma}^{-1}_1 =  \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \,, \\
        \bs{D} & = \left( \bs{\Sigma}^{-1}_2 \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right) - \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \right) \cdot \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} = \left( \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 + \mathbf{I} - \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \right) \cdot \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} = \left( \bs{\Sigma}_1 + \bs{\Sigma}_2 \right)^{-1} \,,
    \end{align*}
    where we used \eqref{eq:Sigma_sum} twice in the second line for $\bs{C}$.
\end{proof}

\section*{Division}
A related theorem that allows us to derive many of our efficient factor update equations is the multi-dimensional Gaussian division theorem.
\begin{theorem}[Gaussian Division Theorem] \label{thm:gaussian_division}
    Given two Gaussian densities $\Gauss{\cdot}{\bs{\tau}_1}{\bs{P}_1}$ and $\Gauss{\cdot}{\bs{\tau}_2}{\bs{P}_2}$ we have
    \begin{align*}
        \frac{\Gauss{\bs{x}}{\bs{\tau}_1}{\bs{P}_1}}{\Gauss{\bs{x}}{\bs{\tau}_2}{\bs{P}_2}} & = \frac{\Gauss{\bs{x}}{\bs{\tau}_1 - \bs{\tau}_2}{\bs{P}_1 - \bs{P}_2}}{\Normal{\bs{\mu}_1}{\bs{\mu}_2}{\bs{\Sigma}_2 - \bs{\Sigma}_1}} \cdot \frac{\left| \bs{\Sigma}_2 \right|}{\left| \bs{\Sigma}_2 - \bs{\Sigma}_1 \right|} \,.
    \end{align*}
\end{theorem}
Thus, when dividing two Gaussian densities, we obtain a non-normalized Gaussian with the precision-mean $\bs{\tau}$ as the difference of the precision-means of the two Gaussians and the precision matrix $\bs{P}$ as the difference of the precision matrices of the two Gaussians. 

\begin{proof}\label{prf:gaussian_division_theorem}
    In order to prove this theorem, we use \eqref{eq:Gauss_definition} to see that 
    \begin{align*}
        \frac{\Gauss{\bs{x}}{\bs{\tau}_1 - \bs{\tau}_2}{\bs{P}_1 - \bs{P}_2} \cdot \Gauss{\bs{x}}{\bs{\tau}_2}{\bs{P}_2}}{\Gauss{\bs{x}}{\bs{\tau}_1}{\bs{P}_1}} & = \left(2\pi\right)^{-\frac{n}{2}} \underbrace{\frac{\left|\bs{P}_1 - \bs{P}_2\right|^{\frac{1}{2}}}{\left|\bs{P}_1\right|^{\frac{1}{2}} \cdot \left|\bs{P}_2\right|^{-\frac{1}{2}}}}_S \cdot \underbrace{\frac{\exp \left( -\frac{1}{2} \left( \transpose{\left( \bs{\tau}_1 - \bs{\tau}_2 \right)} \left( \bs{P}_1 - \bs{P}_2 \right)^{-1} \left( \bs{\tau}_1 - \bs{\tau}_2 \right) \right) \right)}{\exp \left( -\frac{1}{2} \left( \transpose{\bs{\tau}_1} \bs{P}^{-1}_1 \bs{\tau}_1 - \transpose{\bs{\tau}_2} \bs{P}^{-1}_2 \bs{\tau}_2 \right) \right)}}_T 
    \end{align*}
    It remains to show that this equals $\Normal{\bs{\mu}_1}{\bs{\mu}_2}{\bs{\Sigma}_2 - \bs{\Sigma}_1} \cdot \frac{\left| \bs{\Sigma}_2 - \bs{\Sigma}_1 \right|}{\left| \bs{\Sigma}_2 \right|}$ which, using \eqref{eq:Normal_definition} requires us to show that $S =  \left|\bs{\Sigma}_2 - \bs{\Sigma}_1\right|^{\frac{1}{2}} \left| \bs{\Sigma}_2 \right|^{-1}$ and $T = \exp \left( -\frac{1}{2} \transpose{(\bs{\mu}_1-\bs{\mu}_2)} \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} (\bs{\mu}_1-\bs{\mu}_2) \right)$ to prove Theorem \ref{thm:gaussian_division}. First, we see that
    \begin{align*}
        S & = \left| \bs{P}_1^{-1} \cdot \left( \bs{P}_1 - \bs{P}_2 \right) \cdot \bs{P}_2 \right|^{\frac{1}{2}} = \left| \left( \mathbf{I} - \bs{P}_1^{-1} \bs{P}_2 \right) \cdot \bs{P}_2 \right|^{\frac{1}{2}} = \left| \left( \bs{P}_2^{-1} - \bs{P}_1^{-1} \right) \cdot \bs{P}_2^2 \right|^{\frac{1}{2}} = \left| \bs{\Sigma}_2 - \bs{\Sigma}_1 \right|^{\frac{1}{2}} \cdot \left| \bs{\Sigma}_2 \right|^{-1}\,,
    \end{align*}
    where the last step follows from $\bs{P}^{-1} = \bs{\Sigma}$ (see \eqref{eq:Gauss_Normal_transformation}). Using \eqref{eq:Normal_Gauss_transformation} and \eqref{eq:Gauss_Normal_transformation} we see that $\transpose{\bs{\tau}} \bs{P}^{-1} \bs{\tau} = \transpose{\bs{\mu}} \bs{\Sigma}^{-1} \bs{\mu}$ and $\bs{P}_1 - \bs{P}_2 = \bs{\Sigma}^{-1}_2 \cdot \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right) \cdot \bs{\Sigma}^{-1}_1$ which allows us to rewrite $T$ as
    \begin{align*}
        T & = \exp \left( -\frac{1}{2} \left( \transpose{\bs{\mu}_2} \bs{\Sigma}^{-1}_2 \bs{\mu}_2 - \transpose{\bs{\mu}_1} \bs{\Sigma}^{-1}_1 \bs{\mu}_1 + \transpose{\left( \bs{\Sigma}^{-1}_1 \bs{\mu}_1 - \bs{\Sigma}^{-1}_2 \bs{\mu}_2 \right)} \bs{\Sigma}_1 \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \bs{\Sigma}_2 \left( \bs{\Sigma}^{-1}_1 \bs{\mu}_1 - \bs{\Sigma}^{-1}_2 \bs{\mu}_2 \right) \right) \right) \\
        & = \exp \left( -\frac{1}{2} \left( \transpose{\bs{\mu}_1} \bs{A} \bs{\mu}_1 - \transpose{\bs{\mu}_1} \bs{B} \bs{\mu}_2 - \transpose{\bs{\mu}_2} \bs{C} \bs{\mu}_1 + \transpose{\bs{\mu}_2} \bs{D} \bs{\mu}_2 \right) \right) \,,
    \end{align*}
    where the matrices $\bs{A}$, $\bs{B}$, $\bs{C}$, and $\bs{D}$ are given by
    \begin{align*}
        \bs{A} & = \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 -\bs{\Sigma}^{-1}_1 \,, \\
        \bs{B} & = \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \,, \\
        \bs{C} & = \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 \,, \\
        \bs{D} & = \bs{\Sigma}^{-1}_2 + \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \,. 
    \end{align*}
    All we need to show is $\bs{A} = \bs{B} = \bs{C} = \bs{D} = \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1}$ to prove the theorem. To show this, we note that 
    \begin{align}
        \bs{\Sigma}_2 - \bs{\Sigma}_1 = -\bs{\Sigma}_1 \left( \bs{\Sigma}_2^{-1} - \bs{\Sigma}_1^{-1} \right) \bs{\Sigma}_2 = -\bs{\Sigma}_2 \left( \bs{\Sigma}_2^{-1} - \bs{\Sigma}_1^{-1} \right) \bs{\Sigma}_1 \,. \label{eq:Sigma_diff}
    \end{align}
    Thus, we finally have
    \begin{align*}
        \bs{A} & = \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \cdot \left( \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 - \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right) \bs{\Sigma}^{-1}_1 \right) = \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \cdot \left( \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 - \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 + \mathbf{I} \right) = \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \,, \\
        \bs{C} & = -\bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \bs{\Sigma}_1^{-1} \left( \bs{\Sigma}_2^{-1} - \bs{\Sigma}_1^{-1} \right)^{-1} \bs{\Sigma}_2^{-1} \bs{\Sigma}_2 \bs{\Sigma}^{-1}_1 = -\bs{\Sigma}^{-1}_2 \left( \bs{\Sigma}_2^{-1} - \bs{\Sigma}_1^{-1} \right)^{-1} \bs{\Sigma}^{-1}_1 =  \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \,, \\
        \bs{D} & = \left( \bs{\Sigma}^{-1}_2 \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right) + \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \right) \cdot \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} = \left( \mathbf{I} - \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 + \bs{\Sigma}^{-1}_2 \bs{\Sigma}_1 \right) \cdot \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} = \left( \bs{\Sigma}_2 - \bs{\Sigma}_1 \right)^{-1} \,,
    \end{align*}
    where we used \eqref{eq:Sigma_diff} twice in the second line for $\bs{C}$.
\end{proof}

\end{document}
